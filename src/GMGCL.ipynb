{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858ce54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "def set_global_determinism(seed):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=42)\n",
    "\n",
    "from hyperopt import hp\n",
    "import pandas as pd\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.csgraph import laplacian\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "import networkx as nx\n",
    "from datasets import Dataset\n",
    "import layers\n",
    "import metrics\n",
    "import time\n",
    "\n",
    "print(\"tf.__version__: {}\".format(tf.__version__) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeaf576",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, session, dataset, **config):\n",
    "        # tensorflow Session object\n",
    "        self.session = session\n",
    "        # datasets.Dataset object\n",
    "        self.dataset = dataset\n",
    "        # dict of hyperparameters, etc.\n",
    "        self.config = config\n",
    "\n",
    "        # inputs\n",
    "        self.id = tf.placeholder(tf.int64, [None])\n",
    "        self.user_id = tf.placeholder(tf.int64, [None])\n",
    "        self.item_id= tf.placeholder(tf.int64, [None])\n",
    "        # labels\n",
    "        self.r_true = tf.placeholder(tf.float32, [None])\n",
    "        # initialize graph\n",
    "        self.user_mask = self._mask(self.dataset.side_info.get('user_graph', None))\n",
    "        self.item_mask = self._mask(self.dataset.side_info.get('item_graph', None))\n",
    "        rating_threshold = 3.0\n",
    "        self.user_item_mask = self._user_item_mask(self.dataset.data, rating_threshold)\n",
    "        \n",
    "        # define model parameters\n",
    "        self.weights, self.biases, self.user_factor_pp_dense, self.user_feature_dense, self.item_factor_pp_dense, self.item_feature_dense, self.user_factor_in_ui_dense, self.item_factor_in_ui_dense = self._params()\n",
    "        # define rating computation and scale to dataset range\n",
    "        self.r_pred = self.dataset.min + self.dataset.range*tf.sigmoid(self._r_pred())\n",
    "        # define loss\n",
    "        self.loss = self._loss()\n",
    "        # define rmse metric for update monitoring\n",
    "        self.rmse = tf.reduce_mean((self.r_pred - self.r_true)**2)**0.5\n",
    "        \n",
    "        # Adam optimization with default learning rate\n",
    "        self.opt = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "        # acc\n",
    "        self.r_pred_round = tf.round(self.r_pred)\n",
    "        self.correct_prediction = tf.equal(self.r_pred_round, self.r_true)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "    def _params(self):\n",
    "        weights, biases = {}, {}\n",
    "        \n",
    "        user_factor_pp_dense = None\n",
    "        user_feature_dense = None\n",
    "        \n",
    "        item_factor_pp_dense = None\n",
    "        item_feature_dense = None\n",
    "        \n",
    "        user_factor_in_ui_dense = None\n",
    "        item_factor_in_ui_dense = None\n",
    "        \n",
    "        return weights, biases, user_factor_pp_dense, user_feature_dense, item_factor_pp_dense, item_feature_dense, user_factor_in_ui_dense, item_factor_in_ui_dense\n",
    "\n",
    "    def _r_pred(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _local_pred(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _global_local_contra_loss(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _global_local_mse_loss(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def _loss(self):\n",
    "        #recommend task related loss\n",
    "        self.mse = tf.reduce_mean((self.r_pred - self.r_true)**2)\n",
    "        self.infonce_1 = self.infonce_loss(self.user_factor, self.item_factor, self.r_true,)\n",
    "        \n",
    "        self.mse_1 = tf.reduce_mean((self.user_factor_pp_dense - self.user_feature_dense)**2)\n",
    "        self.mse_2 = tf.reduce_mean((self.item_factor_pp_dense - self.item_feature_dense)**2)\n",
    "        self.mse_3 = tf.reduce_mean((self.user_factor_in_ui_dense - self.user_feature_dense)**2)\n",
    "        self.mse_4 = tf.reduce_mean((self.item_factor_in_ui_dense - self.item_feature_dense)**2)\n",
    "        \n",
    "        self.user_content_contra_loss, self.item_content_contra_loss, self.user_action_contra_loss, self.item_action_contra_loss = self._global_local_contra_loss()\n",
    "        self.global_local_contra_loss = self.user_content_contra_loss+self.item_content_contra_loss+self.user_action_contra_loss+self.item_action_contra_loss\n",
    "        \n",
    "        # KL divergence loss\n",
    "        self.global_pred = tf.sigmoid(self._r_pred()) \n",
    "        self.local_pred = tf.sigmoid(self._local_pred()) \n",
    "        self.global_pred_reshape = tf.reshape(self.global_pred, [-1, 1])\n",
    "        self.local_pred_reshape = tf.reshape(self.local_pred, [-1, 1])\n",
    "        self.global_pred_all = tf.concat([self.global_pred_reshape, 1 - self.global_pred_reshape], axis = 1)\n",
    "        self.local_pred_all = tf.concat([self.local_pred_reshape, 1 - self.local_pred_reshape], axis = 1)\n",
    "        self.KL = tf.reduce_mean(\n",
    "                    tf.reduce_sum(self.global_pred_all * tf.math.log(self.global_pred_all / self.local_pred_all),\n",
    "                                  keepdims=True, axis=1)\n",
    "                  )\n",
    "\n",
    "        self.reg = self._reg()\n",
    "        self.alpha = float(self.config.get('alpha', 0))\n",
    "        KL_loss_weight = 0.001\n",
    "        infonce_loss_weight = 0.0001\n",
    "\n",
    "        return (1 - self.alpha)*self.mse/self.dataset.range**2 + \\\n",
    "                        self.alpha*self.reg + self.mse_1 + self.mse_2 + self.mse_3 + self.mse_4 + \\\n",
    "                        infonce_loss_weight*self.global_local_contra_loss + self.infonce_1\n",
    "\n",
    "\n",
    "    def _reg(self):\n",
    "        user_graph = self.dataset.side_info.get('user_graph', None)\n",
    "        item_graph = self.dataset.side_info.get('item_graph', None)\n",
    "        \n",
    "        self.reg_l2, self.reg_graph = 0, 0\n",
    "        for w in self.weights.values():\n",
    "            self.reg_l2 += tf.reduce_sum(w**2)\n",
    "            if w.shape[0] == self.dataset.n_user:\n",
    "                self.reg_graph += self._graph_reg(user_graph, w)\n",
    "            elif w.shape[0] == self.dataset.n_item:\n",
    "                self.reg_graph += self._graph_reg(item_graph, w)\n",
    "            else:\n",
    "                self.reg_graph += self._graph_reg(None, w)\n",
    "        \n",
    "        self.beta = float(self.config.get('beta', 0))\n",
    "        return (1 - self.beta)*self.reg_l2 + self.beta*self.reg_graph\n",
    "\n",
    "    def _graph_reg(self, g, w):\n",
    "        if g is None:\n",
    "            # if no graph is provided, use L2 reg\n",
    "            return tf.reduce_sum(w**2)\n",
    "        if len(w.shape) == 1:\n",
    "            w = tf.reshape(w, (-1, 1))\n",
    "        normed = self.config.get('normed', False)\n",
    "        if self.config.get('sparse', True):\n",
    "            s = laplacian(sp.coo_matrix(g), normed=normed).astype(np.float32)\n",
    "            s = tf.sparse.reorder(tf.SparseTensor(np.array([s.row, s.col]).T, s.data, s.shape))\n",
    "            return tf.linalg.trace(tf.matmul(w, tf.sparse.matmul(s, w), transpose_a=True))\n",
    "        s = tf.constant(laplacian(g.A if sp.issparse(g) else g, normed=normed), dtype=tf.float32)\n",
    "        return tf.linalg.trace(tf.matmul(w, tf.matmul(s, w), transpose_a=True))\n",
    "\n",
    "    def run(self, ops, batch):\n",
    "        return self.session.run(ops, {\n",
    "            self.id: batch.index,\n",
    "            self.user_id: batch.user_id,\n",
    "            self.item_id: batch.item_id,\n",
    "            self.r_true: batch.rating,\n",
    "        })\n",
    "\n",
    "    def train(self, max_updates=100000, n_check=100, patience=float('inf'), batch_size=None):\n",
    "        self.session.run(tf.global_variables_initializer())\n",
    "        best = {'updates': 0, 'loss': float('inf'), 'rmse_tune': float('inf'), 'acc_tune': float('inf'), 'rmse_test': float('inf'), 'acc_test': float('inf')}\n",
    "        for i in range(max_updates):\n",
    "            opt, loss, accuracy = self.run([self.opt, self.loss, self.accuracy], self.dataset.get_batch(mode='train', size=batch_size))\n",
    "            if i % n_check == 0 or i == max_updates - 1:\n",
    "                # monitoring\n",
    "                rmse_tune, acc_tune = self.run([self.rmse,self.accuracy], self.dataset.get_batch(mode='tune', size=102400))\n",
    "                if len(self.dataset.tune) == 0 or rmse_tune < best['rmse_tune']:\n",
    "                    rmse_test, acc_test = self.run([self.rmse, self.accuracy], self.dataset.get_batch(mode='test', size=102400))\n",
    "                    best = {'updates': i, 'loss': loss, 'rmse_tune': rmse_tune, 'acc_tune': acc_tune, 'rmse_test': rmse_test, \"acc_test\": acc_test}\n",
    "                print(best)\n",
    "                if (i - best['updates'])//n_check > patience:\n",
    "                    # early stopping\n",
    "                    break\n",
    "        return best\n",
    "\n",
    "    def test_infer(self):\n",
    "        print(\"\\n\\nTESTING...\")\n",
    "        time_start=time.time()\n",
    "        if self.dataset.name in [\"MovieLens1M\", \"KuaiRandPure\"]:\n",
    "            print(\"batch infer\")\n",
    "            self.dataset.index['test'] = 0\n",
    "            test_pred = []\n",
    "            while True:\n",
    "                tmp_test_dataset = self.dataset.get_batch(mode='test', size=102400)\n",
    "                tmp_test_pred = self.run(self.r_pred, tmp_test_dataset)\n",
    "                test_pred.append( tmp_test_pred )\n",
    "                if self.dataset.index['test']==0:\n",
    "                    break\n",
    "                \n",
    "            test_pred = np.hstack(test_pred)\n",
    "        else:\n",
    "            test_dataset = self.dataset.get_batch(mode='test', size=None)\n",
    "            test_pred = self.run(self.r_pred, test_dataset)\n",
    "        test_dataset = self.dataset.get_batch(mode='test', size=None)\n",
    "        r_true, user_ids, item_ids = test_dataset[['rating', 'user_id', 'item_id']].values.T\n",
    "        print('\\ttime cost',time.time()-time_start,'s')\n",
    "        return {'test_pred': test_pred, 'r_true': r_true, 'user_ids': user_ids, 'item_ids': item_ids}\n",
    "\n",
    "\n",
    "    def get_metrics(self, r_pred):\n",
    "        print(\"\\n\\nEVALUATING...\")\n",
    "        time_start=time.time()\n",
    "        r_pred, r_true, user_ids, item_ids = r_pred['test_pred'], r_pred['r_true'], r_pred['user_ids'], r_pred['item_ids']\n",
    "\n",
    "        _metric1 = metrics.metric_at_once(r_pred, r_true, user_ids, item_ids, k=10)\n",
    "        _metric2 = metrics.metric_at_once(r_pred, r_true, user_ids, item_ids, k=20)\n",
    "        dddall = {**_metric1, **_metric2}\n",
    "        \n",
    "        return_res = {\n",
    "            \"P@10\":dddall['Precision@10'],\"P@20\":dddall['Precision@20'],\n",
    "            \"R@10\":dddall['Recall@10'],\"R@20\":dddall['Recall@20'],\n",
    "            \"H@10\":dddall['Hitrate@10'],\"H@20\":dddall['Hitrate@20'],\n",
    "            \"N@10\":dddall['NDCG@10'],\"N@20\":dddall['NDCG@20']\n",
    "                     }\n",
    "        return_res = {k:round(v,6) for k,v in return_res.items()}\n",
    "        \n",
    "        print('\\ttime cost',time.time()-time_start,'s')\n",
    "        return return_res\n",
    "\n",
    "\n",
    "    def _schema(self):\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases,\n",
    "        }\n",
    "\n",
    "    def _mask(self, g):\n",
    "        # define the mask used for GAT\n",
    "        if g is None:\n",
    "            # no masking\n",
    "            return 1.0\n",
    "        if self.config.get('sparse', True):\n",
    "            shape = g.shape\n",
    "            g = sp.coo_matrix(g, shape=shape, dtype=np.float32)\n",
    "            g = tf.sparse.reorder(tf.SparseTensor(np.array([g.row, g.col]).T, g.data, shape))\n",
    "            return tf.sparse.add(tf.sparse.eye(*shape), g)\n",
    "        return tf.eye(*g.shape) + tf.constant(g.A if sp.issparse(g) else g, dtype=tf.float32)\n",
    "    \n",
    "    def _user_item_mask(self, input_df, rating_threshold):\n",
    "        ratings = input_df[[\"user_id\",\"item_id\",\"rating\"]]\n",
    "        # WARNING: if the number of user is greater than 10000, you should modify the following code:\n",
    "        ratings_itemid = ratings[\"item_id\"] + 10000\n",
    "        ratings_userid = ratings[\"user_id\"]\n",
    "        ratings.loc[:, \"item_id\"] = ratings_itemid\n",
    "        ratings.loc[:, \"user_id\"] = ratings_userid\n",
    "        G = nx.Graph()\n",
    "\n",
    "        print(\"\\n\\nGRAPH BUILDING\")\n",
    "        print(\"\\tuserNum: {} itemNum: {}\".format(len(set(ratings[\"user_id\"])), len(set(ratings[\"item_id\"]))))\n",
    "        if self.dataset.diff_tag_n_user:\n",
    "            print(\"\\tbuild gragh: fix user num from {} to {}\".format(len(set(ratings[\"user_id\"])), self.dataset.n_user))\n",
    "            G.add_nodes_from(list(range(self.dataset.n_user)), bipartite=0)\n",
    "        else:\n",
    "            G.add_nodes_from(ratings[\"user_id\"], bipartite=0)\n",
    "            \n",
    "        if self.dataset.diff_tag_n_item:\n",
    "            print(\"\\tbuild gragh: fix item num from {} to {}\".format(len(set(ratings[\"item_id\"])), self.dataset.n_item))\n",
    "            G.add_nodes_from(list(range(10000, self.dataset.n_item + 10000)), bipartite=0)\n",
    "        else:\n",
    "            G.add_nodes_from(ratings[\"item_id\"], bipartite=1)\n",
    "        \n",
    "        # set threshold\n",
    "        valid_df = ratings[ratings[\"rating\"] > rating_threshold][[\"user_id\", \"item_id\"]]\n",
    "        edge_list = valid_df.apply(lambda x: tuple(x), axis=1).values.tolist()\n",
    "\n",
    "        G.add_edges_from(edge_list)\n",
    "\n",
    "        A = np.array(nx.adjacency_matrix(G, nodelist=list(G.nodes).sort()).todense())\n",
    "        return A\n",
    "\n",
    "    def _normalized_aggregation(self, g, w):\n",
    "        if g is None:\n",
    "            return 0.0\n",
    "        if self.config.get('sparse', True):\n",
    "            g = sp.coo_matrix(g, dtype=np.float32)\n",
    "            g = tf.sparse.reorder(tf.SparseTensor(np.array([g.row, g.col]).T, g.data, g.shape))\n",
    "            return tf.sparse.matmul(g, w)/(tf.sparse.reduce_sum(g, axis=1, keepdims=True)**0.5 + 1e-10)\n",
    "        g = tf.constant(g.A if sp.issparse(g) else g, dtype=tf.float32)\n",
    "        return tf.matmul(g, w)/(tf.reduce_sum(g, axis=1, keepdims=True)**0.5 + 1e-10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984cf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMGCL(Model):\n",
    "\n",
    "    def _r_pred(self):\n",
    "        return (tf.reduce_sum(self.user_factor*self.item_factor, 1)\n",
    "            + self.user_bias + self.item_bias + self.bias)\n",
    "    \n",
    "    def infonce_loss(self, user_emb, item_emb, r_true, rou = 1, SCORE_THRESHOLD=3.0):\n",
    "        user_emb = tf.math.l2_normalize(user_emb, axis=-1)  # [n, dim]\n",
    "        item_emb = tf.math.l2_normalize(item_emb, axis=-1)  # [n, dim]\n",
    "        \n",
    "        pos_inner_product = tf.math.reduce_sum(user_emb * item_emb, keepdims=True, axis=1)\n",
    "        numerator = tf.math.exp(pos_inner_product / rou)\n",
    "        # negative sample\n",
    "        negative_score_mask = tf.where(r_true>SCORE_THRESHOLD, tf.ones_like(numerator), tf.zeros_like(numerator))\n",
    "        numerator = numerator * negative_score_mask\n",
    "\n",
    "        all_inner_product = tf.matmul(user_emb, item_emb, transpose_b=True)\n",
    "\n",
    "        eye_matrix = (1-tf.eye(tf.shape(user_emb)[0]))\n",
    "        positive_score_mask  = tf.where(r_true<=SCORE_THRESHOLD, tf.ones_like(numerator), tf.zeros_like(numerator))\n",
    "        positive_score_mask = tf.math.logical_or(tf.cast(eye_matrix, tf.bool), tf.cast(positive_score_mask, tf.bool))\n",
    "        positive_score_mask = tf.cast(positive_score_mask, tf.float32)\n",
    "\n",
    "        denominator_tmp_local = positive_score_mask * tf.math.exp(all_inner_product / rou)\n",
    "        denominator = tf.math.reduce_sum(denominator_tmp_local, keepdims=True, axis=1)\n",
    "        \n",
    "        return tf.reduce_mean(-tf.log(numerator / denominator+0.0001))\n",
    "\n",
    "\n",
    "    def _params(self):\n",
    "        # basic params\n",
    "        self.rank = int(self.config.get('rank', 1))\n",
    "        weights = {\n",
    "            'user_factor': tf.get_variable('user_factor', [self.dataset.n_user, self.rank]),\n",
    "            'item_factor': tf.get_variable('item_factor', [self.dataset.n_item, self.rank]),\n",
    "            'user_bias': tf.Variable(tf.zeros([self.dataset.n_user])),\n",
    "            'item_bias': tf.Variable(tf.zeros([self.dataset.n_item])),\n",
    "        }\n",
    "        biases = {'bias': tf.Variable(0.0)}\n",
    "        self.user_factor = tf.nn.embedding_lookup(weights['user_factor'], self.user_id)\n",
    "        self.item_factor = tf.nn.embedding_lookup(weights['item_factor'], self.item_id)\n",
    "        self.user_bias = tf.nn.embedding_lookup(weights['user_bias'], self.user_id)\n",
    "        self.item_bias = tf.nn.embedding_lookup(weights['item_bias'], self.item_id)\n",
    "        self.bias = biases['bias']\n",
    "        \n",
    "    \n",
    "        data, shape = self.dataset.data, (self.dataset.n_user, self.dataset.n_item)\n",
    "        implicit = sp.coo_matrix((data.is_train, (data.user_id, data.item_id)), shape=shape)\n",
    "\n",
    "        self.k = int(self.config.get('k', 1))\n",
    "        u, s, vt = svds(implicit.astype(float), k=self.k)\n",
    "        \n",
    "        self.user_features = tf.constant(u*s**0.5, dtype=tf.float32)\n",
    "        self.item_features = tf.constant(vt.T*s**0.5, dtype=tf.float32)\n",
    "\n",
    "        user_features = self.dataset.side_info.get('user_features', None)\n",
    "        item_features = self.dataset.side_info.get('item_features', None)\n",
    "         \n",
    "        \n",
    "        if user_features is not None:\n",
    "            self.user_features = tf.concat(\n",
    "                [self.user_features, tf.constant(user_features.values, dtype=tf.float32)], 1)\n",
    "        if item_features is not None:\n",
    "            self.item_features = tf.concat(\n",
    "                [self.item_features, tf.constant(item_features.values, dtype=tf.float32)], 1)\n",
    "            \n",
    "        user_feature_shape_dim = self.user_features.shape[-1]\n",
    "        item_feature_shape_dim = self.item_features.shape[-1]\n",
    "\n",
    "        self.n_head = int(self.config.get('n_head', 1))\n",
    "        self.activation_in = tf.keras.activations.get(self.config.get('activation_in', 'softsign'))\n",
    "        self.activation_out = tf.keras.activations.get(self.config.get('activation_out', 'hard_sigmoid'))\n",
    "        self.residual = bool(self.config.get('residual', True))\n",
    "        self.user_in = layers.GAT(\n",
    "            self.user_features.shape[1], self.n_head*self.rank, 1, concat=False, residual=self.residual, name='user_in')\n",
    "        self.user_out = layers.Dense(self.user_in.dim_out, self.rank, name='user_out')\n",
    "\n",
    "        self.user_pp_dense = layers.Dense(self.rank, self.rank, name='user_pp_dense')\n",
    "        self.user_feat_dense = layers.Dense(user_feature_shape_dim, self.rank, name='user_feat_dense')\n",
    "     \n",
    "        self.item_pp_dense = layers.Dense(self.rank, self.rank, name='item_pp_dense')\n",
    "        self.item_feat_dense = layers.Dense(item_feature_shape_dim, self.rank, name='item_feat_dense')\n",
    "\n",
    "        \n",
    "        self.item_in = layers.GAT(\n",
    "            self.item_features.shape[1], self.n_head*self.rank, 1, concat=False, residual=self.residual, name='item_in')\n",
    "        self.item_out = layers.Dense(self.item_in.dim_out, self.rank, name='item_out')\n",
    "        \n",
    "        self.user_feat_in = layers.Dense(user_feature_shape_dim, self.rank, name='user_feat_in')\n",
    "        self.item_feat_in = layers.Dense(item_feature_shape_dim, self.rank, name='item_feat_in')\n",
    "        self.user_item_in = layers.GAT(\n",
    "            self.rank, self.n_head*self.rank, 1, concat=False, residual=self.residual, name='user_item_in')\n",
    "        self.user_item_out = layers.Dense(self.user_item_in.dim_out, self.rank, name='user_item_out')\n",
    "        self.user_factor_in_ui_graph_dense = layers.Dense(self.rank, self.rank, name='user_factor_in_ui_graph_dense')\n",
    "        self.item_factor_in_ui_graph_dense = layers.Dense(self.rank, self.rank, name='item_factor_in_ui_graph_dense')\n",
    "\n",
    "        self.user_gate = layers.Dense(3 * self.rank, self.rank, name='user_gate')\n",
    "        self.item_gate = layers.Dense(3 * self.rank, self.rank, name='item_gate')\n",
    "        self.uu_view_dense = layers.Dense(2 * self.rank, self.rank, name='uu_view_dense')\n",
    "        self.ui_view_dense = layers.Dense(2 * self.rank, self.rank, name='ui_view_dense')\n",
    "        self.ii_view_dense = layers.Dense(2 * self.rank, self.rank, name='ii_view_dense')\n",
    "        self.iu_view_dense = layers.Dense(2 * self.rank, self.rank, name='iu_view_dense')\n",
    "       \n",
    "        self.user_global_view_dense = layers.Dense(self.rank, self.rank, name='user_global_view_dense')\n",
    "        self.item_global_view_dense = layers.Dense(self.rank, self.rank, name='item_global_view_dense')\n",
    "        \n",
    "        self.user_content_local_dense = layers.Dense(self.rank, self.rank, name='user_content_local_dense')\n",
    "        self.user_action_local_dense = layers.Dense(self.rank, self.rank, name='user_action_local_dense')\n",
    "        self.user_global_dense = layers.Dense(self.rank, self.rank, name='user_global_dense')\n",
    "        \n",
    "        self.item_content_local_dense = layers.Dense(self.rank, self.rank, name='item_content_local_dense')\n",
    "        self.item_action_local_dense = layers.Dense(self.rank, self.rank, name='item_action_local_dense')\n",
    "        self.item_global_dense = layers.Dense(self.rank, self.rank, name='item_global_dense')\n",
    "        \n",
    "        for layer in [self.user_in, self.user_out, self.item_in, self.item_out, self.user_pp_dense, self.user_feat_dense, self.item_pp_dense, self.item_feat_dense, self.user_gate, self.item_gate,\\\n",
    "                      self.user_feat_in, self.item_feat_in, self.user_item_in, self.user_item_out, self.user_factor_in_ui_graph_dense, self.item_factor_in_ui_graph_dense,\\\n",
    "                      self.uu_view_dense, self.ui_view_dense, self.ii_view_dense, self.iu_view_dense,\\\n",
    "                      self.user_global_view_dense, self.item_global_view_dense,\\\n",
    "                      self.user_content_local_dense, self.user_action_local_dense, self.user_global_dense, self.item_content_local_dense, self.item_action_local_dense, self.item_global_dense]:\n",
    "            weights.update(layer.get_weights())\n",
    "            biases.update(layer.get_biases())\n",
    "\n",
    "        sparse = bool(self.config.get('sparse', True))\n",
    "        \n",
    "        self.user_factor_pp = self.user_out(self.user_in(\n",
    "            self.user_features, self.activation_in, self.user_mask, sparse), self.activation_out)\n",
    "        \n",
    "        user_factor_pp_dense = self.user_pp_dense(self.user_factor_pp, self.activation_out)\n",
    "        print('self.user_factor_pp_dense.shape:', user_factor_pp_dense.shape)\n",
    "        user_feature_dense = self.user_feat_dense(self.user_features, self.activation_out)\n",
    "        \n",
    "        self.item_factor_pp = self.item_out(self.item_in(\n",
    "            self.item_features, self.activation_in, self.item_mask, sparse), self.activation_out)\n",
    "        \n",
    "        item_factor_pp_dense = self.item_pp_dense(self.item_factor_pp, self.activation_out)\n",
    "        print('self.item_factor_pp_dense.shape:', item_factor_pp_dense.shape)\n",
    "        item_feature_dense = self.item_feat_dense(self.item_features, self.activation_out)\n",
    "        \n",
    "        user_feat_in_dense = self.user_feat_in(self.user_features)\n",
    "        item_feat_in_dense = self.item_feat_in(self.item_features)\n",
    "        self.user_item_in_dense = tf.concat([user_feat_in_dense, item_feat_in_dense], axis = 0)  # [m+n, dim]\n",
    "        self.user_item_factor_pp = self.user_item_out(self.user_item_in(\n",
    "            self.user_item_in_dense, self.activation_in, self.user_item_mask, sparse), self.activation_out)\n",
    "        self.user_factor_in_ui_graph = tf.slice(self.user_item_factor_pp, [0, 0], [tf.shape(self.user_features)[0], -1])\n",
    "        self.item_factor_in_ui_graph = tf.slice(self.user_item_factor_pp, [tf.shape(self.user_features)[0], 0], [-1, -1])\n",
    "        user_factor_in_ui_dense = self.user_factor_in_ui_graph_dense(self.user_factor_in_ui_graph, self.activation_out)\n",
    "        item_factor_in_ui_dense = self.item_factor_in_ui_graph_dense(self.item_factor_in_ui_graph, self.activation_out)\n",
    "        \n",
    "        weight1_fix = 1.0\n",
    "        weight2_fix = 1.0\n",
    "        weight3_fix = 1.0\n",
    "        use_fix_weight = True\n",
    "        if use_fix_weight:\n",
    "            self.user_global_view = weight1_fix*weights['user_factor'] + weight2_fix*self.user_factor_pp + weight3_fix*self.user_factor_in_ui_graph\n",
    "            self.item_global_view = weight1_fix*weights['item_factor'] + weight2_fix*self.item_factor_pp + weight3_fix*self.item_factor_in_ui_graph\n",
    "\n",
    "        self.user_factor = tf.nn.embedding_lookup(self.user_global_view, self.user_id)\n",
    "        self.item_factor = tf.nn.embedding_lookup(self.item_global_view, self.item_id)\n",
    "   \n",
    "\n",
    "        self.user_factor_local = tf.nn.embedding_lookup(self.user_factor_pp, self.user_id)\n",
    "        self.item_factor_local = tf.nn.embedding_lookup(self.item_factor_pp, self.item_id)\n",
    "        \n",
    "\n",
    "        cosine_threshold = 0.5\n",
    "        self.user_factor_norm = tf.math.l2_normalize(self.user_global_view, axis=-1)\n",
    "        self.user_factor_cos = tf.matmul(self.user_factor_norm, self.user_factor_norm, transpose_b = True)\n",
    "        self.user_mask = tf.where(self.user_factor_cos > cosine_threshold, self.user_mask, tf.zeros_like(self.user_mask))\n",
    "\n",
    "        self.item_factor_norm = tf.math.l2_normalize(self.item_global_view, axis=-1)\n",
    "        self.item_factor_cos = tf.matmul(self.item_factor_norm, self.item_factor_norm, transpose_b = True)\n",
    "        self.item_mask = tf.where(self.item_factor_cos > cosine_threshold, self.item_mask, tf.zeros_like(self.item_mask))\n",
    "        \n",
    "        user_item_factor_norm_concat = tf.concat([self.user_factor_norm, self.item_factor_norm], axis=0)\n",
    "        self.user_item_factor_cos = tf.matmul(user_item_factor_norm_concat, user_item_factor_norm_concat, transpose_b = True)\n",
    "        self.user_item_mask = tf.where(self.user_item_factor_cos > cosine_threshold, self.user_item_mask, tf.zeros_like(self.user_item_mask))\n",
    "            \n",
    "        return weights, biases, user_factor_pp_dense, user_feature_dense, item_factor_pp_dense, item_feature_dense, user_factor_in_ui_dense, item_factor_in_ui_dense\n",
    "\n",
    "    def _schema(self):\n",
    "        return {\n",
    "            'weights': self.weights,\n",
    "            'biases': self.biases,\n",
    "            'outputs': {\n",
    "                'user_alpha': self.user_in.heads[0].alpha,\n",
    "                'item_alpha': self.item_in.heads[0].alpha,\n",
    "                'user_factor_pp': self.user_factor_pp,\n",
    "                'item_factor_pp': self.item_factor_pp,\n",
    "                'user_factor': self.user_weight1 * self.weights['user_factor'] + self.user_weight2 * self.user_factor_pp,\n",
    "                'item_factor': self.item_weight1 * self.weights['item_factor'] + self.item_weight2 * self.item_factor_pp,\n",
    "            },\n",
    "        }\n",
    "    \n",
    "    def _local_pred(self):\n",
    "        return (tf.reduce_sum(self.user_factor_local*self.item_factor_local, 1)\n",
    "            + self.user_bias + self.item_bias + self.bias)\n",
    "    \n",
    "    def _cal_contra_loss_v1(self, local_view, global_view, rou = 0.07):\n",
    "        pos_inner_product = tf.math.reduce_sum(local_view * global_view, keepdims=True, axis=1)\n",
    "        numerator = tf.math.exp(pos_inner_product / rou)\n",
    "        \n",
    "        all_inner_product = tf.matmul(local_view, global_view, transpose_b=True)\n",
    "        denominator_tmp = tf.math.exp(all_inner_product / rou)\n",
    "        denominator = tf.math.reduce_sum(denominator_tmp, keepdims=True, axis=1)\n",
    "        \n",
    "        return tf.reduce_mean(-tf.log(numerator / denominator))\n",
    "    \n",
    "    def _global_local_contra_loss(self):\n",
    "        rou = 0.07\n",
    "        \n",
    "        user_content_local_norm = tf.math.l2_normalize(self.weights['user_factor']+self.user_factor_pp, axis=-1)  # [m, dim]\n",
    "        user_action_local_norm = tf.math.l2_normalize(self.weights['user_factor']+self.user_factor_in_ui_graph, axis=-1)  # [m, dim]\n",
    "        user_global_view_norm = self.user_factor_norm  # [m, dim]\n",
    "        \n",
    "        item_content_local_norm = tf.math.l2_normalize(self.weights['item_factor']+self.item_factor_pp, axis=-1)  # [n, dim]\n",
    "        item_action_local_norm = tf.math.l2_normalize(self.weights['item_factor']+self.item_factor_in_ui_graph, axis=-1)  # [n, dim]\n",
    "        item_global_view_norm = self.item_factor_norm  # [n, dim]\n",
    "\n",
    "        user_content_contra_loss = self._cal_contra_loss_v1(user_content_local_norm, user_global_view_norm, rou)\n",
    "        item_content_contra_loss = self._cal_contra_loss_v1(item_content_local_norm, item_global_view_norm, rou)\n",
    "\n",
    "        user_action_contra_loss = self._cal_contra_loss_v1(user_action_local_norm, user_global_view_norm, rou)\n",
    "        item_action_contra_loss = self._cal_contra_loss_v1(item_action_local_norm, item_global_view_norm, rou)\n",
    "        \n",
    "        return user_content_contra_loss, item_content_contra_loss, user_action_contra_loss, item_action_contra_loss\n",
    "    \n",
    "    def _global_local_mse_loss(self):\n",
    "        user_content_local = self.user_content_local_dense(self.weights['user_factor']+self.user_factor_pp, self.activation_out)\n",
    "        user_action_local = self.user_action_local_dense(self.weights['user_factor']+self.user_factor_in_ui_graph, self.activation_out)\n",
    "        user_global = self.user_global_dense(self.weights['user_factor']+self.user_factor_pp+self.user_factor_in_ui_graph, self.activation_out)\n",
    "        \n",
    "        item_content_local = self.item_content_local_dense(self.weights['item_factor']+self.item_factor_pp, self.activation_out)\n",
    "        item_action_local = self.item_action_local_dense(self.weights['item_factor']+self.item_factor_in_ui_graph, self.activation_out)\n",
    "        item_global = self.item_global_dense(self.weights['item_factor']+self.item_factor_pp+self.item_factor_in_ui_graph, self.activation_out)\n",
    "\n",
    "        user_content_mse_loss = tf.reduce_mean((user_content_local - user_global)**2)\n",
    "        item_content_mse_loss = tf.reduce_mean((item_content_local - item_global)**2)\n",
    "\n",
    "        user_action_mse_loss = tf.reduce_mean((user_action_local - user_global)**2)\n",
    "        item_action_mse_loss = tf.reduce_mean((item_action_local - item_global)**2)\n",
    "        \n",
    "        return user_content_mse_loss + item_content_mse_loss + user_action_mse_loss + item_action_mse_loss\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83a01c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    MODEL = GMGCL\n",
    "    \n",
    "    # choose dataset\n",
    "    # DATASET_PATH = '../data/datasets/MovieLens100K' # check data/datasets for options\n",
    "    # DATASET_PATH = '../data/datasets/Flixster' # check data/datasets for options\n",
    "    DATASET_PATH = '../data/datasets/MovieLens1M' # check data/datasets for options\n",
    "    # DATASET_PATH = '../data/datasets/KuaiRandPure' # check data/datasets for options\n",
    "    \n",
    "    BATCH_SIZE=512\n",
    "    DATASET = Dataset.load(DATASET_PATH)\n",
    "    NAME = '{}_{}'.format(MODEL.__name__, DATASET.name)\n",
    "    \n",
    "    config = {'activation_in': 'softsign', 'activation_out': 'sigmoid', 'alpha': 1e-05, \n",
    "              'k': 16.0, 'n_head': 5.0, 'rank': 12.0, 'residual': True, 'max_updates': 1000, 'sparse': False}\n",
    "\n",
    "    #NOTE(yangfei05):\n",
    "    config['sparse'] = False\n",
    "    print(f\"parameters: {config}\")\n",
    "\n",
    "    print(\"\\n\\nTRAIN AND EVAL\")\n",
    "    data = DATASET.data[['user_id', 'item_id', 'rating', 'is_test']]\n",
    "            \n",
    "    dataset = Dataset(data, **DATASET.side_info)\n",
    "    dataset_np = np.array(dataset)\n",
    "\n",
    "    \n",
    "    physical_devices = tf.config.list_physical_devices('GPU') \n",
    "    for gpu_instance in physical_devices: \n",
    "        tf.config.experimental.set_memory_growth(gpu_instance, True)\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as session:\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            session.run(tf.local_variables_initializer())\n",
    "            model = MODEL(session, dataset, **config)\n",
    "            print('model:', model)\n",
    "            model.train(max_updates=config['max_updates'], batch_size=BATCH_SIZE)\n",
    "            test_pred_df = model.test_infer()\n",
    "            score = model.get_metrics(test_pred_df)\n",
    "            print(score)\n",
    "            display( pd.DataFrame({k:[v] for k,v in score.items()}) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2a397c0fe47d9645f2161f8f89b80facf1e14459c3cd0f838949e1f4226576e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
